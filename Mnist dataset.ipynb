{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /home/pranav/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead set\n",
      "data_dir=gs://tfds-data/datasets.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbce2981f044f73b281041fafa6d784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to /home/pranav/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset ,mnist_info = tfds.load(name ='mnist', with_info=True, as_supervised =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train ,mnist_test = mnist_dataset['train'] , mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = .1 * mnist_info.splits['train'].num_examples # could be int or not\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples ,tf.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image ,label):\n",
    "    image = tf.cast(image ,tf.float32)\n",
    "    image /= 255  #to converting val bw 0 and 1 \n",
    "    return image , label\n",
    "scaled_train_validate_data = mnist_train.map(scale)  #data_set.map(fn) -> fn(data_set)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE  =10000   #shuffling 10000 samples at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_validate_data = scaled_train_validate_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_validate_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_validate_data.skip(num_validation_samples)\n",
    "\n",
    "#Batching gradient descent\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data =validation_data.batch(num_validation_samples)  # single batch of total validaton set\n",
    "test_data =test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784  #first layer\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape =(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHoosing optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer ='adam' ,loss ='sparse_categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 13s - loss: 0.3309 - accuracy: 0.9052 - val_loss: 0.1612 - val_accuracy: 0.9562\n",
      "Epoch 2/5\n",
      "540/540 - 12s - loss: 0.1340 - accuracy: 0.9600 - val_loss: 0.1074 - val_accuracy: 0.9693\n",
      "Epoch 3/5\n",
      "540/540 - 12s - loss: 0.0938 - accuracy: 0.9717 - val_loss: 0.0815 - val_accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "540/540 - 12s - loss: 0.0739 - accuracy: 0.9777 - val_loss: 0.0767 - val_accuracy: 0.9775\n",
      "Epoch 5/5\n",
      "540/540 - 12s - loss: 0.0585 - accuracy: 0.9816 - val_loss: 0.0612 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f326247df10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHs = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHs,validation_data = (validation_inputs,validation_targets),validation_steps=10, verbose =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 2s 2s/step - loss: 0.0804 - accuracy: 0.9754"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08042366802692413, 0.9754)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
